{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THEORITICAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. ***What is Logistic Regression, and how does it differ from Linear Regression.***\n",
    "*Answer-*\n",
    "\n",
    "\n",
    "#### **1. Logistic Regression**\n",
    "Logistic Regression is a statistical method used for **classification problems**, particularly binary classification (where the target variable has two possible outcomes, e.g., Yes/No, Spam/Not Spam). Instead of predicting a continuous value, it estimates the **probability** that a given input belongs to a particular class.\n",
    "\n",
    "- It uses the **logistic (sigmoid) function** to transform the output into a probability ranging from 0 to 1:\n",
    "  \n",
    "  \\[\n",
    "P(Y=1∣X) = 1/1+e−(β0+β1X)\n",
    "  \\]\n",
    "\n",
    "- The decision boundary is determined by setting a threshold (e.g., 0.5):  \n",
    "  - If \\(P(Y=1∣X)≥0.5\\), classify as 1\n",
    " \n",
    "  - If \\( P(Y=1 | X) < 0.5 \\), classify as 0  \n",
    "\n",
    "- Uses **Maximum Likelihood Estimation (MLE)** for optimization instead of minimizing squared errors.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Linear Regression**\n",
    "Linear Regression is a technique used for **regression problems**, where the goal is to predict a **continuous** output based on input features.\n",
    "\n",
    "- It assumes a linear relationship between the independent variable(s) (\\(X\\)) and the dependent variable (\\(Y\\)):\n",
    "\n",
    "  \\[\n",
    "  Y=β0+β1X+ϵ\n",
    "  \\]\n",
    "\n",
    "- Uses **Ordinary Least Squares (OLS)** to minimize the sum of squared errors:\n",
    "\n",
    "  \\[\n",
    "  min∑(Y−Y^)2\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Differences**\n",
    "| Feature            | Logistic Regression | Linear Regression |\n",
    "|--------------------|--------------------|--------------------|\n",
    "| **Type of Problem** | Classification (e.g., Yes/No) | Regression (e.g., Predicting prices) |\n",
    "| **Output** | Probability (0 to 1) | Continuous numeric value |\n",
    "| **Mathematical Function** | Sigmoid function | Linear equation |\n",
    "| **Optimization Method** | Maximum Likelihood Estimation (MLE) | Ordinary Least Squares (OLS) |\n",
    "| **Decision Boundary** | Threshold-based (e.g., 0.5) | Direct continuous value |\n",
    "| **Interpretation** | Coefficients represent log-odds | Coefficients represent rate of change |\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "- Use **Logistic Regression** when you need to classify data into categories.\n",
    "- Use **Linear Regression** when predicting numerical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ***What is the mathematical equation of Logistic Regression.***\n",
    "*Answer-*\n",
    "\n",
    "The mathematical equation for Logistic Regression is based on the logistic (sigmoid) function, which transforms a linear equation into a probability.\n",
    "\n",
    "1. Sigmoid Function\n",
    "The sigmoid function is defined as:\n",
    "\n",
    "- 𝜎(𝑧)= 1/1+𝑒−𝑧\n",
    "\n",
    "where z is a linear combination of the input features:\n",
    "\n",
    "- z = 𝛽0+𝛽1𝑋1+𝛽2𝑋2+⋯+𝛽𝑛𝑋𝑛\n",
    "\n",
    "Thus, the logistic regression model is given by:\n",
    "\n",
    "-   𝑃(𝑌=1∣𝑋) = 1/1+𝑒−(𝛽0+𝛽1𝑋1+𝛽2𝑋2+⋯+𝛽𝑛𝑋𝑛)\n",
    "\n",
    "where:\n",
    "-   P(Y=1∣X) is the probability that the output Y is 1 given input X.\n",
    "\n",
    "-   β0 (intercept) and 𝛽1,𝛽2,...,𝛽𝑛(coefficients) are the parameters learned from the data.\n",
    "\n",
    "-   𝑋1,𝑋2,...,𝑋𝑛\n",
    "are the input features.\n",
    "\n",
    "2. Log-Odds (Logit Function) :\n",
    "Taking the logit (log-odds) transformation of the probability:-\n",
    "\n",
    "-   log(𝑃(𝑌=1∣𝑋)/1−𝑃(𝑌=1∣𝑋)) = 𝛽0+𝛽1𝑋1+𝛽2𝑋2+⋯+𝛽𝑛𝑋𝑛\n",
    "\n",
    "This equation shows that logistic regression models the log-odds of the dependent variable as a linear function of the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ***Why do we use the Sigmoid function in Logistic Regression.***\n",
    "*Answer-*\n",
    "\n",
    "In **Logistic Regression**, the Sigmoid function is used because it maps the output of the linear regression model to a probability between 0 and 1, allowing us to interpret the model's prediction as the likelihood of a binary event occurring, which is the core functionality of logistic regression.\n",
    "\n",
    "#### **Key points about using Sigmoid in Logistic Regression:**\n",
    "\n",
    "• Probability Interpretation: The Sigmoid function outputs a value always between 0 and 1, which directly translates to a probability value, making it ideal for predicting binary outcomes like \"yes/no\" or \"fraudulent/not fraudulent\".  \n",
    "• Smooth Gradient: The Sigmoid function has a smooth gradient, which is crucial for efficient training using gradient descent optimization algorithms.  \n",
    "• Mathematical Convenience: The derivative of the Sigmoid function is easily calculable, facilitating the backpropagation process during model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ***What is the cost function of Logistic Regression.***\n",
    "*Answer-*\n",
    "The cost function for logistic regression is the `log loss, or binary cross-entropy loss`. It's used to measure how well a model fits data by comparing predicted probabilities to actual class labels.\n",
    "\n",
    "#### **How it works-**\n",
    "\n",
    "-   The cost function is the average of the log loss over all training examples. \n",
    "-   The cost function penalizes incorrect predictions more heavily. \n",
    "-   The cost function decreases to 0 as the probability gets closer to the true value. \n",
    "-   The cost function is designed to optimize the parameters to minimize the prediction error.\n",
    "\n",
    "#### **Why it's used-**\n",
    "\n",
    "-   The cost function is used to determine how well a model fits the data. \n",
    "-   The cost function is used to quantify the performance of the model. \n",
    "\n",
    "#### **How to minimize it-**\n",
    "\n",
    "-   The cost function can be minimized by adjusting the model parameters. \n",
    "-   The cost function can be minimized using the gradient descent algorithm.\n",
    "\n",
    "#### **Related concepts-**\n",
    "\n",
    "-   The cost function should align with the model's objective and the nature of the target variable. \n",
    "-   The cost function is a classification evaluation metric that is used to compare different models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. ***What is Regularization in Logistic Regression? Why is it needed.***\n",
    "*Answer-*\n",
    "\n",
    "In Logistic Regression, `\"Regularization\"` is a technique used to prevent overfitting by adding a penalty term to the loss function, effectively shrinking the model coefficients and forcing it to learn more generalizable patterns from the data, rather than memorizing specific details of the training set, which is crucial for making accurate predictions on new data. \n",
    "\n",
    "#### **Key points about regularization in Logistic Regression:**\n",
    "\n",
    "• Overfitting problem: \n",
    "-   When a model learns the training data too well, it can perform poorly on unseen data due to overfitting. \n",
    "\n",
    "• Penalty term:\n",
    "-   Regularization adds a penalty term to the loss function that increases as the model coefficients become larger, encouraging the model to learn smaller weights.  \n",
    "\n",
    "#### **Why is regularization needed?**\n",
    "\n",
    "• Improved generalization: \n",
    "-   By preventing overfitting, regularization helps the model to generalize better to new data points.\n",
    "• Handling high-dimensional data: \n",
    "-   When dealing with a large number of features, regularization can help prevent the model from assigning too much importance to irrelevant features.\n",
    "• Controlling model complexity: \n",
    "-   Regularization acts as a mechanism to control the complexity of the model by limiting the magnitude of the coefficients.  \n",
    "\n",
    "#### **Common types of regularization in Logistic Regression:**\n",
    "\n",
    "• L1 Regularization (Lasso): \n",
    "-   Sum of absolute values of the coefficients, often leads to feature selection by setting some coefficients to zero.\n",
    "\n",
    "• L2 Regularization (Ridge): \n",
    "-   Sum of squared coefficients, tends to shrink coefficients towards zero without necessarily setting them to zero. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. ***Explain the difference between Lasso, Ridge, and Elastic Net regression.***\n",
    "*Answer-*\n",
    "\n",
    "Lasso, Ridge, and Elastic Net are all regression techniques that can improve the performance of a linear model. They are all regularization methods that use a penalty term to shrink coefficients and control sparsity. \n",
    "\n",
    "#### **`Lasso: (L1 Regularization)`**\n",
    "\n",
    "-   Selects variables by setting some coefficients to zero, effectively performing feature selection..\n",
    "-   Can reduce overfitting in a linear model.\n",
    "-   Tends to eliminate one of a group of correlated terms.\n",
    "- **Best for:-** When expect that only a few predictors are important, and want to automatically select them.\n",
    "\n",
    "#### **`Ridge: (L2 Regularization)`**\n",
    "\n",
    "-   Reduces the impact of multicollinearity.\n",
    "-   Shrinks all coefficients for correlated variables together.\n",
    "-   Never sets coefficients to zero, so it doesn't perform feature selection.\n",
    "-   **Best for:-** Situations where many predictors contribute small effects, and want to reduce their impact without eliminating them.\n",
    "\n",
    "#### **`Elastic Net: (Combination of L1 and L2 Regularization)`**\n",
    "\n",
    "-   Combines the strengths of Lasso and Ridge.\n",
    "-   Shrinks some coefficients to zero and others towards zero.\n",
    "-   Groups and shrinks parameters associated with correlated variables.\n",
    "-   Balances between ridge and lasso regression.\n",
    "-   **Best for:-** When we have correlated predictors and want a mix of feature selection (Lasso) and coefficient shrinkage (Ridge).\n",
    "\n",
    "The trade-off between coefficient shrinkage and sparsity control is controlled by a parameter called `alpha`. When alpha is zero, elastic net regression reduces to ridge regression. When alpha is one, elastic net regression reduces to lasso regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. ***When should we use Elastic Net instead of Lasso or Ridge?***\n",
    "*Answer-*\n",
    "\n",
    "We should use **`Elastic Net`** instead of `Lasso or Ridge` in the following scenarios:-\n",
    "\n",
    "### i. **When we Have Highly Correlated Predictors (Multicollinearity)**  \n",
    "   - **Lasso** tends to arbitrarily select one feature and ignore others, which may lead to unstable model selection when predictors are highly correlated.  \n",
    "   - **Elastic Net** avoids this issue by using a mix of L1 and L2 regularization, allowing correlated variables to share importance rather than completely dropping one.\n",
    "\n",
    "### ii. **When we Need Feature Selection but Lasso is Too Aggressive**  \n",
    "   - **Lasso** can sometimes remove too many features, leading to underfitting.  \n",
    "   - **Elastic Net** balances feature selection with Ridge’s ability to retain small but useful coefficients.\n",
    "\n",
    "### iii. **When we Have More Predictors Than Observations (High-Dimensional Data)**  \n",
    "   - **Lasso** struggles in high-dimensional settings and may select too few variables.  \n",
    "   - **Elastic Net** performs better by stabilizing selection and distributing weights more effectively across correlated predictors.\n",
    "\n",
    "### iv. **When Ridge Alone is Too Weak for Sparsity**  \n",
    "   - **Ridge** retains all variables but shrinks their coefficients. If you want some variables to be eliminated while still maintaining regularization, **Elastic Net** is a better choice.\n",
    "\n",
    "### **Best Practices:**\n",
    "- **Use Ridge** when all variables contribute and multicollinearity is present.  \n",
    "- **Use Lasso** when you expect only a few important variables.  \n",
    "- **Use Elastic Net** when you need a balance between Ridge and Lasso, especially when variables are correlated.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. ***What is the impact of the regularization parameter (λ) in Logistic Regression.***\n",
    "*Answer-*\n",
    "\n",
    "In **Logistic Regression**, the regularization parameter \\( \\lambda \\) (often represented as **C** in scikit-learn, where \\( C = \\frac{1}{\\lambda} \\)) controls the strength of the penalty applied to the model coefficients. Its impact depends on whether **L1 (Lasso), L2 (Ridge), or Elastic Net** regularization is used.\n",
    "\n",
    "### **Effects of \\( lambda \\) on Logistic Regression**\n",
    "1. **High \\( \\lambda \\) (Strong Regularization, Low C)**\n",
    "   - Shrinks the magnitude of coefficients towards zero.\n",
    "   - Reduces model complexity and helps prevent overfitting.\n",
    "   - In **L1 (Lasso) regularization**, some coefficients may be forced to zero, leading to feature selection.\n",
    "   - In **L2 (Ridge) regularization**, coefficients are reduced but not eliminated.\n",
    "\n",
    "2. **Low \\( \\lambda \\) (Weak Regularization, High C)**\n",
    "   - Allows the model to fit more closely to the training data.\n",
    "   - Can lead to overfitting if the data is noisy.\n",
    "   - In extreme cases (\\(\\lambda = 0\\)), the model is equivalent to standard Logistic Regression without penalty.\n",
    "\n",
    "3. **Optimal \\( \\lambda \\) (Balanced Regularization)**\n",
    "   - Finds a trade-off between bias and variance.\n",
    "   - Helps improve generalization and stability in predictions.\n",
    "\n",
    "### **Choosing \\( \\lambda \\)**\n",
    "- Typically, **cross-validation** (e.g., GridSearchCV) is used to tune \\( \\lambda \\) for the best performance.\n",
    "- In **sklearn**, the parameter **C** is used instead of \\( \\lambda \\), where **higher C means weaker regularization**:\n",
    "  ```python\n",
    "  from sklearn.linear_model import LogisticRegression\n",
    "  model = LogisticRegression(C=0.1, penalty='l2')  # Stronger regularization\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. ***What are the key assumptions of Logistic Regression?***\n",
    "*Answer-*\n",
    "\n",
    "Logistic Regression is a widely used classification algorithm, but it relies on several key assumptions for optimal performance. Unlike Linear Regression, it does **not** assume a linear relationship between the independent and dependent variables but has its own set of requirements. Here are the key assumptions:  \n",
    "\n",
    "---\n",
    "\n",
    "### **i. No Multicollinearity**  \n",
    "- The independent variables should not be highly correlated with each other.  \n",
    "- High correlation (multicollinearity) makes it difficult to estimate the contribution of each predictor.  \n",
    "- **Fix:** Use **Variance Inflation Factor (VIF)** to detect multicollinearity and remove or combine correlated features.  \n",
    "\n",
    "---\n",
    "\n",
    "### **ii. Linearity of Independent Variables with Log-Odds**  \n",
    "- Logistic Regression does not assume a linear relationship between **features and output**, but it assumes a linear relationship between **independent variables and the log-odds (logit transformation). in another words there is a linear relationship between the independent variables and the log-odds of the dependent variable.**\n",
    "- **Fix:** If this assumption is violated, try **polynomial terms**, **log transformation**, or use **tree-based models** instead.\n",
    "\n",
    "---\n",
    "\n",
    "### **iii. No Extreme Outliers**  \n",
    "- Outliers can **distort** the coefficients since Logistic Regression uses Maximum Likelihood Estimation (MLE).  \n",
    "- **Fix:** Detect outliers using **box plots, Z-scores, or IQR method**, and remove or transform them.\n",
    "\n",
    "---\n",
    "\n",
    "### **iv. Independence of Observations (No Autocorrelation)**  \n",
    "- Observations should be independent of each other, meaning one data point should not influence another (especially important in time-series data).  \n",
    "- **Fix:** If autocorrelation exists (e.g., in time-series data), use **time-lagged variables** or methods like **Generalized Estimating Equations (GEE)**.\n",
    "\n",
    "---\n",
    "\n",
    "### **v. Sufficient Sample Size (Large Dataset)**  \n",
    "- Logistic Regression performs best with a **large dataset** because Maximum Likelihood Estimation (MLE) requires sufficient data to converge to stable coefficients.  \n",
    "- If the dataset is too small, the model may overfit or give unstable results.  \n",
    "- **Fix:** Use **regularization (L1/L2)**, collect more data, or consider **Bayesian Logistic Regression**.\n",
    "\n",
    "---\n",
    "\n",
    "### **When These Assumptions Are Violated**  \n",
    "If the data does not meet these assumptions:  \n",
    "✔ **Regularization (Ridge, Lasso, Elastic Net) to handle multicollinearity.**  \n",
    "✔ **Tree-based models (e.g., Decision Trees, Random Forest, XGBoost) for nonlinear relationships.**  \n",
    "✔ **Generalized Additive Models (GAMs) to relax linearity assumptions.**  \n",
    "✔ **Resampling methods if data is imbalanced (e.g., SMOTE, undersampling).**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. ***What are some alternatives to Logistic Regression for classification tasks ?***\n",
    "*Answer-*\n",
    "\n",
    "There are several alternatives to **Logistic Regression** for classification tasks, each with its own strengths and weaknesses. Here are some key alternatives:\n",
    "\n",
    "---\n",
    "\n",
    "### **`i. Decision Trees`** 🌳  \n",
    "✅ **Pros:**  \n",
    "- Handles non-linear relationships and interactions between variables.  \n",
    "- No need for feature scaling or transformation.  \n",
    "- Works well with both numerical and categorical data.  \n",
    "\n",
    "❌ **Cons:**  \n",
    "- Prone to overfitting (can be mitigated with pruning or depth limitation).  \n",
    "- Can be unstable if the data changes slightly.  \n",
    "\n",
    "**Best for:** When interpretability is important and the data has non-linear patterns.  \n",
    "\n",
    "---\n",
    "\n",
    "### **`ii. Random Forest`** 🌲🌲  \n",
    "✅ **Pros:**  \n",
    "- Ensemble method that reduces overfitting.  \n",
    "- Handles missing values and outliers well.  \n",
    "- Works well with large datasets and mixed data types.  \n",
    "\n",
    "❌ **Cons:**  \n",
    "- Less interpretable than a single decision tree.  \n",
    "- Computationally expensive for large datasets.  \n",
    "\n",
    "**Best for:** When you need a robust, accurate model with minimal tuning.  \n",
    "\n",
    "---\n",
    "\n",
    "### **`iii. Gradient Boosting (XGBoost, LightGBM, CatBoost)`** ⚡  \n",
    "✅ **Pros:**  \n",
    "- Very high accuracy, especially for structured/tabular data.  \n",
    "- Handles missing values and categorical features (CatBoost).  \n",
    "- Can model complex relationships with feature interactions.  \n",
    "\n",
    "❌ **Cons:**  \n",
    "- Computationally expensive.  \n",
    "- Requires careful hyperparameter tuning.  \n",
    "\n",
    "**Best for:** When you need **state-of-the-art** performance for structured data.  \n",
    "\n",
    "---\n",
    "\n",
    "### **`iv. Support Vector Machines (SVM)`** 🎯  \n",
    "✅ **Pros:**  \n",
    "- Works well with high-dimensional data.  \n",
    "- Effective for small-to-medium datasets with clear class boundaries.  \n",
    "- Can use different **kernel functions** to handle non-linearity (e.g., RBF, polynomial).  \n",
    "\n",
    "❌ **Cons:**  \n",
    "- Computationally expensive for large datasets.  \n",
    "- Hard to interpret compared to Logistic Regression.  \n",
    "\n",
    "**Best for:** When you have high-dimensional data and need a powerful classifier.  \n",
    "\n",
    "---\n",
    "\n",
    "### **`v. k-Nearest Neighbors (k-NN)`** 📍  \n",
    "✅ **Pros:**  \n",
    "- Simple, easy to understand, and non-parametric.  \n",
    "- No training phase—just store the data and classify based on neighbors.  \n",
    "- Works well for small datasets.  \n",
    "\n",
    "❌ **Cons:**  \n",
    "- Slow for large datasets (since it needs to search neighbors for each prediction).  \n",
    "- Sensitive to irrelevant features and the choice of \\( k \\).  \n",
    "\n",
    "**Best for:** When you have **small datasets** and want an easy-to-implement model.  \n",
    "\n",
    "---\n",
    "\n",
    "### **`vi. Neural Networks (Deep Learning - MLP, CNN, RNN)`** 🧠  \n",
    "✅ **Pros:**  \n",
    "- Can model highly complex, non-linear relationships.  \n",
    "- Scales well with large datasets and GPU acceleration.  \n",
    "- Useful for images, text, and sequential data.  \n",
    "\n",
    "❌ **Cons:**  \n",
    "- Requires large amounts of labeled data.  \n",
    "- Computationally expensive and harder to interpret.  \n",
    "\n",
    "**Best for:** When you have a large dataset with complex relationships (e.g., image recognition, NLP).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Choosing the Right Alternative**\n",
    "| Model | Handles Non-Linearity? | Works with Small Data? | Handles High-Dimensional Data? | Interpretability |\n",
    "|--------|------------------|------------------|------------------------|---------------|\n",
    "| Logistic Regression | ❌ No | ✅ Yes | ✅ Yes | ✅ High |\n",
    "| Decision Tree | ✅ Yes | ✅ Yes | ❌ No | ✅ High |\n",
    "| Random Forest | ✅ Yes | ✅ Yes | ✅ Yes | ❌ Moderate |\n",
    "| Gradient Boosting (XGBoost, LightGBM) | ✅ Yes | ✅ Yes | ✅ Yes | ❌ Low |\n",
    "| SVM | ✅ Yes (with kernel) | ✅ Yes | ✅ Yes | ❌ Low |\n",
    "| k-NN | ✅ Yes | ✅ Yes | ❌ No | ✅ High |\n",
    "| Neural Networks | ✅ Yes | ❌ No | ✅ Yes | ❌ Low |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. ***What are Classification Evaluation Metrics ?***\n",
    "*Answer-*\n",
    "\n",
    "### **Classification Evaluation Metrics**  \n",
    "When evaluating a classification model, several metrics help measure its **accuracy, precision, recall, and overall effectiveness**. The right metric depends on the specific use case and the importance of **false positives vs. false negatives**.\n",
    "\n",
    "---\n",
    "\n",
    "### **`i. Accuracy`**\n",
    "\\[\n",
    "Accuracy = {TP + TN}/{TP + TN + FP + FN}\n",
    "\\]\n",
    "\n",
    "✅ **Pros:** Easy to understand, good when class distribution is balanced.  \n",
    "❌ **Cons:** Misleading for **imbalanced** datasets (e.g., if 95% of data is one class, a 95% accuracy model could be useless).  \n",
    "\n",
    "👉 **Best for:** Balanced datasets where false positives & false negatives are equally important.  \n",
    "\n",
    "---\n",
    "\n",
    "### **`ii. Precision (Positive Predictive Value, PPV)`**\n",
    "\\[\n",
    "Precision = {TP}/{TP + FP}\n",
    "\\]\n",
    "✅ **Pros:** Measures how many **predicted positives are actually correct**.  \n",
    "❌ **Cons:** Ignores false negatives (missed cases).  \n",
    "\n",
    "👉 **Best for:** When **false positives** are costly (e.g., spam detection, medical diagnosis).  \n",
    "\n",
    "---\n",
    "\n",
    "### **`iii. Recall (Sensitivity, True Positive Rate, TPR)`** \n",
    "\\[\n",
    "Recall = \\frac{TP}{TP + FN}\n",
    "\\]\n",
    "✅ **Pros:** Measures how many **actual positives were correctly identified**.  \n",
    "❌ **Cons:** Ignores false positives (could lead to many incorrect predictions).  \n",
    "\n",
    "👉 **Best for:** When **false negatives** are costly (e.g., detecting cancer, fraud detection).  \n",
    "\n",
    "---\n",
    "\n",
    "### **`iv. F1-Score (Harmonic Mean of Precision & Recall)`**\n",
    "\\[\n",
    "F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\n",
    "\\]\n",
    "✅ **Pros:** Balances precision & recall, useful for imbalanced datasets.  \n",
    "❌ **Cons:** Less interpretable than accuracy.  \n",
    "\n",
    "👉 **Best for:** When **both false positives & false negatives matter** (e.g., NLP, fraud detection).  \n",
    "\n",
    "---\n",
    "\n",
    "### **`v. ROC Curve & AUC (Area Under Curve)`**  \n",
    "- **ROC Curve:** Plots **TPR vs. FPR** at different thresholds.  \n",
    "- **AUC (Area Under Curve):** Measures overall model performance (closer to 1 = better).  \n",
    "\n",
    "✅ **Pros:** Works well even for imbalanced data.  \n",
    "❌ **Cons:** Doesn’t consider actual class distribution.  \n",
    "\n",
    "👉 **Best for:** Comparing different models' ability to rank predictions.  \n",
    "\n",
    "---\n",
    "\n",
    "### **`vi. PR Curve & AUC-PR (Precision-Recall Curve)`**\n",
    "- **PR Curve:** Plots **Precision vs. Recall** at different thresholds.  \n",
    "- **AUC-PR:** Area under the Precision-Recall curve (better for imbalanced datasets).  \n",
    "\n",
    "✅ **Pros:** More informative when dealing with **imbalanced** data.  \n",
    "❌ **Cons:** Less useful for balanced datasets.  \n",
    "\n",
    "👉 **Best for:** Imbalanced data where positive class is rare (e.g., rare disease detection).  \n",
    "\n",
    "---\n",
    "\n",
    "### **`vii. Log Loss (Cross-Entropy Loss):`** \n",
    "\n",
    "✅ **Pros:** Accounts for prediction **confidence**.  \n",
    "❌ **Cons:** Hard to interpret directly.  \n",
    "\n",
    "👉 **Best for:** Probabilistic models where confidence matters (e.g., logistic regression, neural networks).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Choosing the Right Metric**\n",
    "| **Metric** | **Use Case** |\n",
    "|------------|-------------|\n",
    "| **Accuracy** | When data is balanced and misclassification costs are equal |\n",
    "| **Precision** | When false positives are costly (e.g., spam detection, medical diagnosis) |\n",
    "| **Recall** | When false negatives are costly (e.g., fraud detection, cancer detection) |\n",
    "| **F1-Score** | When precision & recall are equally important |\n",
    "| **ROC-AUC** | When evaluating a model’s ability to rank predictions |\n",
    "| **PR-AUC** | When dealing with imbalanced datasets |\n",
    "| **Log Loss** | When predicting probabilities instead of hard classifications |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. ***How does class imbalance affect Logistic Regression?***\n",
    "*Answer-*\n",
    "\n",
    "### **How Class Imbalance Affects Logistic Regression**  \n",
    "Class imbalance occurs when one class is significantly more frequent than the other(s). This can negatively impact **Logistic Regression** in several ways:\n",
    "\n",
    "---\n",
    "\n",
    "### **`i. Biased Model Predictions`**  \n",
    "- Logistic Regression **minimizes overall error**, so it tends to favor the **majority class**.  \n",
    "- This leads to **low recall** for the minority class, meaning the model fails to correctly identify rare cases.  \n",
    "- Example: If 95% of the data is **negative (class 0)** and only 5% is **positive (class 1)**, the model might just predict **class 0** all the time and still achieve **95% accuracy**—but this is meaningless.\n",
    "\n",
    "---\n",
    "\n",
    "### **`ii. Poor Decision Boundary`**  \n",
    "- The model **learns a decision boundary** that favors the majority class.  \n",
    "- It becomes **less sensitive to the minority class**, reducing its ability to correctly classify rare events.  \n",
    "\n",
    "📌 **Example:**  \n",
    "Imagine a fraud detection model where **fraud cases = 1%** and **non-fraud cases = 99%**.  \n",
    "- The model might predict **\"not fraud\"** for every case, achieving **99% accuracy** but missing all actual fraud cases.\n",
    "\n",
    "---\n",
    "\n",
    "### **`iii. Misleading Performance Metrics`**  \n",
    "- **Accuracy is unreliable** in imbalanced datasets.  \n",
    "- **Precision, Recall, F1-score, and AUC-PR are better choices.**  \n",
    "- High **accuracy** does not mean the model is performing well.\n",
    "\n",
    "📌 **Example:**  \n",
    "- A model that predicts **\"no fraud\"** for every transaction in a fraud detection system might have **99% accuracy** but **0% recall** for fraud cases.\n",
    "\n",
    "---\n",
    "\n",
    "### **How to Handle Class Imbalance in Logistic Regression**\n",
    "✅ **`i. Use Better Evaluation Metrics`**  \n",
    "Instead of **accuracy**, use:  \n",
    "- **Precision, Recall, and F1-score**  \n",
    "- **ROC-AUC & Precision-Recall (PR) Curve**  \n",
    "\n",
    "✅ **`ii. Adjust Class Weights`**  \n",
    "Logistic Regression in **scikit-learn** allows class weighting:  \n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(class_weight='balanced')  # Automatically balances classes\n",
    "```\n",
    "This increases the penalty for misclassifying the minority class.\n",
    "\n",
    "✅ **`iii. Resampling Techniques`**  \n",
    "- **Oversampling** the minority class (e.g., **SMOTE** - Synthetic Minority Over-sampling Technique).  \n",
    "- **Undersampling** the majority class (randomly removing samples from the majority class).  \n",
    "- Example using SMOTE:  \n",
    "```python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "X_resampled, y_resampled = SMOTE().fit_resample(X, y)\n",
    "```\n",
    "\n",
    "✅ **`iv. Collect More Data`**  \n",
    "- If possible, **gather more data** for the minority class.  \n",
    "- Helps the model learn patterns better.\n",
    "\n",
    "✅ **`v. Try Different Models`**  \n",
    "- **Tree-based models** (Random Forest, XGBoost) often handle imbalance better.  \n",
    "- **Anomaly detection** techniques may be useful for extreme cases.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways**  \n",
    "- Logistic Regression struggles with imbalanced data and **tends to favor the majority class**.  \n",
    "- **Accuracy is misleading**—use **Precision, Recall, F1-score, and PR-AUC** instead.  \n",
    "- **Use class weights or resampling techniques (SMOTE) to balance data.**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. ***What is Hyperparameter Tuning in Logistic Regression?***\n",
    "*Answer-*\n",
    "\n",
    "`Hyperparameter tuning` is the process of finding the best values for model parameters that are not learned from data but set before training. In Logistic Regression, the key hyperparameters affect **`regularization`**, **`optimization`**, and **`class balancing.`**\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Hyperparameters in Logistic Regression**\n",
    "### **1. Regularization Strength (\\( C \\))** 🔧  \n",
    "- Controls the strength of **L2 (Ridge) or L1 (Lasso) regularization**.  \n",
    "- **C is the inverse of λ** (i.e., \\( C = \\frac{1}{\\lambda} \\)):  \n",
    "  - **High \\( C \\) (weak regularization)** → More complex model (risk of overfitting).  \n",
    "  - **Low \\( C \\) (strong regularization)** → Simpler model (risk of underfitting).  \n",
    "- Example:  \n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(C=0.1)  # Stronger regularization\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Regularization Type (Penalty: L1, L2, or Elastic Net)** 🔗  \n",
    "- **L1 (Lasso)** → Feature selection by forcing some coefficients to **zero**.  \n",
    "- **L2 (Ridge)** → Shrinks coefficients but **does not remove** features.  \n",
    "- **Elastic Net** → Combines L1 & L2 (tunable using `l1_ratio`).  \n",
    "- Example:  \n",
    "```python\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear')  # L1 regularization\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Solver (Optimization Algorithm)** ⚡  \n",
    "- Different solvers optimize Logistic Regression differently:  \n",
    "  - **liblinear** → Good for **small datasets**, supports L1 & L2.  \n",
    "  - **saga** → Handles **large datasets**, supports L1, L2, & Elastic Net.  \n",
    "  - **lbfgs** → Default, works well for most cases but only supports L2.  \n",
    "- Example:  \n",
    "```python\n",
    "model = LogisticRegression(solver='saga', penalty='elasticnet', l1_ratio=0.5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Class Weight (Handling Imbalance)** ⚖️  \n",
    "- `class_weight='balanced'` adjusts weights **inversely proportional to class frequencies**.  \n",
    "- Helps improve recall for minority classes.  \n",
    "- Example:  \n",
    "```python\n",
    "model = LogisticRegression(class_weight='balanced')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Hyperparameter Tuning Techniques**\n",
    "### **1. Grid Search (Exhaustive Search) 🔍**  \n",
    "Tries all possible combinations of hyperparameters and picks the best one.  \n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='f1')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Random Search (Faster but Less Precise) 🎲**  \n",
    "Selects random combinations instead of trying all possibilities.  \n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "param_dist = {'C': uniform(0.01, 10)}\n",
    "random_search = RandomizedSearchCV(LogisticRegression(), param_dist, n_iter=10, cv=5, scoring='f1')\n",
    "random_search.fit(X_train, y_train)\n",
    "print(random_search.best_params_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Bayesian Optimization (Smart Search) 🧠**  \n",
    "Uses past evaluations to find the best parameters efficiently. Libraries like **Optuna** or **Hyperopt** can be used for this.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "| **Hyperparameter** | **Effect** |\n",
    "|--------------------|-----------|\n",
    "| `C` | Controls regularization strength (low C = stronger regularization) |\n",
    "| `penalty` | L1 (feature selection), L2 (shrinkage), Elastic Net (both) |\n",
    "| `solver` | Optimization algorithm (e.g., liblinear, saga, lbfgs) |\n",
    "| `class_weight` | Handles class imbalance (`balanced` gives equal importance to all classes) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. ***What are different solvers in Logistic Regression? Which one should be used?***\n",
    "*Answer-*\n",
    "\n",
    "#### **Different Solvers in Logistic Regression & When to Use Them** ⚡  \n",
    "\n",
    "In **Scikit-Learn**, the `solver` parameter in `LogisticRegression()` determines which **optimization algorithm** is used to minimize the loss function. The choice of solver impacts **speed, accuracy, and compatibility with regularization types**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **i. liblinear (Good for Small Datasets & L1 Regularization):**\n",
    "- Uses **Coordinate Descent (CD) + Trust-Region** optimization.  \n",
    "- Supports **L1 & L2 regularization** (but not Elastic Net).  \n",
    "- Works **only for binary classification** (not recommended for multiclass tasks).  \n",
    "- **Slower** for large datasets.  \n",
    "\n",
    "✅ **Best for:**  \n",
    "✔ Small datasets  \n",
    "✔ Sparse data  \n",
    "✔ When using **L1 (Lasso) regularization**  \n",
    "\n",
    "❌ **Avoid if:** The dataset is large or requires Elastic Net.  \n",
    "\n",
    "🔹 **Example Usage:**  \n",
    "```python\n",
    "model = LogisticRegression(solver='liblinear', penalty='l1', C=1.0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **ii. lbfgs (Default, Best for Large Datasets & Multiclass):**\n",
    "- Uses **Limited-memory BFGS (LBFGS) algorithm** (a variant of Newton’s method).  \n",
    "- Supports **only L2 regularization** (no L1 or Elastic Net).  \n",
    "- Works well for **multiclass classification** (softmax regression).  \n",
    "- **Fast and memory-efficient** for large datasets.  \n",
    "\n",
    "✅ **Best for:**  \n",
    "✔ **Large datasets**  \n",
    "✔ **Multiclass classification (one-vs-rest & multinomial)**  \n",
    "✔ When **L2 regularization** is sufficient  \n",
    "\n",
    "❌ **Avoid if:** You need L1 or Elastic Net regularization.  \n",
    "\n",
    "🔹 **Example Usage:**  \n",
    "```python\n",
    "model = LogisticRegression(solver='lbfgs', multi_class='multinomial', C=1.0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **iii. saga (Best for Large Datasets, Supports L1, L2, & Elastic Net):**\n",
    "- Stochastic Average Gradient (SAGA) = **SGD + Variance Reduction**.  \n",
    "- Supports **L1, L2, and Elastic Net regularization**.  \n",
    "- Works well for **sparse data & large datasets**.  \n",
    "- Handles **multiclass classification**.  \n",
    "\n",
    "✅ **Best for:**  \n",
    "✔ **Very large datasets**  \n",
    "✔ **Sparse datasets (text data, high-dimensional features)**  \n",
    "✔ **L1 (Lasso), L2 (Ridge), or Elastic Net regularization**  \n",
    "\n",
    "❌ **Avoid if:** Your dataset is small (may introduce noise).  \n",
    "\n",
    "🔹 **Example Usage:**  \n",
    "```python\n",
    "model = LogisticRegression(solver='saga', penalty='elasticnet', l1_ratio=0.5, C=1.0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **iv. newton-cg (Best for Multiclass & L2 Regularization):**\n",
    "- Uses **Newton’s Conjugate Gradient (Newton-CG) optimization**.  \n",
    "- Works well for **multiclass classification** (softmax regression).  \n",
    "- **Supports only L2 regularization** (no L1 or Elastic Net).  \n",
    "\n",
    "✅ **Best for:**  \n",
    "✔ **Multiclass classification**  \n",
    "✔ **L2 regularization**  \n",
    "\n",
    "❌ **Avoid if:** You need L1 or Elastic Net regularization.  \n",
    "\n",
    "🔹 **Example Usage:**  \n",
    "```python\n",
    "model = LogisticRegression(solver='newton-cg', multi_class='multinomial', C=1.0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **v. sag (Best for Large Datasets, Only L2 Regularization):**\n",
    "- **Stochastic Average Gradient (SAG)** = Variant of **Stochastic Gradient Descent (SGD)**.  \n",
    "- Only supports **L2 regularization**.  \n",
    "- Works well for **large datasets** with **many features**.  \n",
    "\n",
    "✅ **Best for:**  \n",
    "✔ **Very large datasets**  \n",
    "✔ **L2 regularization**  \n",
    "\n",
    "❌ **Avoid if:** You need L1 or Elastic Net.  \n",
    "\n",
    "🔹 **Example Usage:**  \n",
    "```python\n",
    "model = LogisticRegression(solver='sag', C=1.0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Which Solver Should You Use? 🤔**\n",
    "| **Solver** | **Supports L1?** | **Supports L2?** | **Supports Elastic Net?** | **Best For** | **Avoid If** |\n",
    "|------------|-----------------|-----------------|-----------------|-------------|------------|\n",
    "| **liblinear** | ✅ Yes | ✅ Yes | ❌ No | Small datasets, L1 regularization | Large datasets, multiclass |\n",
    "| **lbfgs** (default) | ❌ No | ✅ Yes | ❌ No | Large datasets, multiclass | Need L1 or Elastic Net |\n",
    "| **saga** | ✅ Yes | ✅ Yes | ✅ Yes | Large/sparse datasets, all regularization types | Small datasets |\n",
    "| **newton-cg** | ❌ No | ✅ Yes | ❌ No | Multiclass, large datasets | Need L1 or Elastic Net |\n",
    "| **sag** | ❌ No | ✅ Yes | ❌ No | Large datasets, fast optimization | Need L1 or Elastic Net |\n",
    "\n",
    "---\n",
    "\n",
    "### **TL;DR: Best Solver for Different Cases**\n",
    "- **For small datasets & L1 regularization:** `liblinear`\n",
    "- **For large datasets & L2 regularization:** `lbfgs` or `sag`\n",
    "- **For large datasets & L1/L2/Elastic Net:** `saga`\n",
    "- **For multiclass classification:** `lbfgs` or `newton-cg`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. ***How is Logistic Regression extended for multiclass classification?***\n",
    "*Answer-*\n",
    " \n",
    "\n",
    "By default, **Logistic Regression** is designed for **binary classification** (i.e., two classes: 0 or 1). However, when we have **more than two classes**, we need to extend it using **multiclass classification strategies**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **i. One-vs-Rest (OvR) or One-vs-All (OvA)**\n",
    "**(Default in Scikit-Learn: `multi_class='auto'`)**  \n",
    "- Trains **one binary logistic regression model per class**.  \n",
    "- Each model predicts **\"Is this class vs. the rest?\"**  \n",
    "- The final prediction is based on the class with the highest probability.  \n",
    "\n",
    "📌 **Example:**  \n",
    "For **three classes: A, B, C**, the model trains:  \n",
    "1. **Model 1:** A vs (B, C)  \n",
    "2. **Model 2:** B vs (A, C)  \n",
    "3. **Model 3:** C vs (A, B)  \n",
    "\n",
    "✅ **Pros:**  \n",
    "✔ Works well with any solver (even `liblinear`).  \n",
    "✔ Efficient for datasets where one class is **rare**.  \n",
    "\n",
    "❌ **Cons:**  \n",
    "- Does **not capture relationships** between classes.  \n",
    "- Can be inconsistent when probabilities overlap.  \n",
    "\n",
    "🔹 **Implementation:**  \n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(multi_class='ovr', solver='liblinear')  # Works with any solver\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **ii. Multinomial (Softmax Regression)**\n",
    "**(Recommended for True Multiclass Problems: `multi_class='multinomial'`)**  \n",
    "- Uses a single **Logistic Regression model** for all classes.  \n",
    "- Instead of multiple binary models, it applies the **Softmax function**. \n",
    "- Predicts **the class with the highest probability**.  \n",
    "\n",
    "📌 **Example:**  \n",
    "For **three classes (A, B, C)**, the model directly estimates:  \n",
    "- \\( P(A) \\), \\( P(B) \\), and \\( P(C) \\) for each input.  \n",
    "\n",
    "✅ **Pros:**  \n",
    "✔ More **consistent probability estimates**.  \n",
    "✔ **Better performance for truly multiclass tasks**.  \n",
    "\n",
    "❌ **Cons:**  \n",
    "- Requires solvers like **lbfgs, saga, or newton-cg** (not `liblinear`).  \n",
    "\n",
    "🔹 **Implementation:**  \n",
    "```python\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')  # Better for true multiclass\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Which Method Should You Use?**\n",
    "| **Method**  | **Pros** | **Cons** | **Best Use Case** |\n",
    "|-------------|---------|---------|----------------|\n",
    "| **One-vs-Rest (OvR)** | Works with any solver, fast training | Doesn't capture class relationships | If binary models make sense (e.g., fraud detection) |\n",
    "| **Multinomial (Softmax)** | More accurate, better probability estimation | Requires advanced solvers | True multiclass classification (e.g., handwritten digit recognition) |\n",
    "\n",
    "---\n",
    "\n",
    "## **TL;DR: When to Use Which?**\n",
    "✅ **If using `liblinear` solver:** Use **OvR**  \n",
    "✅ **If dataset is truly multiclass:** Use **Multinomial (Softmax) with `lbfgs`, `saga`, or `newton-cg`**  \n",
    "✅ **If dataset is small and binary-like:** Use **OvR**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. ***What are the advantages and disadvantages of Logistic Regression?***\n",
    "*Answer-*  \n",
    "\n",
    "Logistic Regression is a simple yet powerful algorithm for classification tasks. Here’s a breakdown of its **pros** and **cons**:  \n",
    "\n",
    "---\n",
    "\n",
    "####  **Advantages of Logistic Regression:-**  \n",
    "\n",
    "#### **i. Simple & Interpretable:**  \n",
    "- **Easy to understand** compared to complex models like Neural Networks.  \n",
    "- Coefficients can be **interpreted as the impact of each feature** on the outcome.  \n",
    "\n",
    "📌 **Example:** In a medical study, a positive weight for \"Smoking\" means smoking **increases** the risk of disease.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **ii. Computationally Efficient:**  \n",
    "- **Fast to train** even on large datasets.  \n",
    "- Works well for **high-dimensional datasets** (with many features).  \n",
    "\n",
    "📌 **Example:** Logistic Regression can train **millions of examples** much faster than deep learning models.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **iii. Works Well for Linearly Separable Data:**  \n",
    "- If classes can be separated with a straight line (decision boundary), **Logistic Regression performs very well**.  \n",
    "\n",
    "📌 **Example:** Spam detection with **email length & number of links** as features.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **iv. Probability Outputs:**  \n",
    "- Provides a **probability score** for class membership.  \n",
    "- Useful for applications where uncertainty is important (e.g., medical diagnosis, credit scoring).  \n",
    "\n",
    "📌 **Example:** A **fraud detection model** predicting a **75% chance of fraud** can trigger a manual review.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **v. Works Well with Feature Selection & Regularization:**  \n",
    "- Supports **L1 (Lasso), L2 (Ridge), and Elastic Net regularization** to **reduce overfitting**.  \n",
    "\n",
    "📌 **Example:** L1 regularization helps select **only the most important features** by setting unimportant ones to zero.  \n",
    "\n",
    "---\n",
    "\n",
    "####  **Disadvantages of Logistic Regression:-**  \n",
    "\n",
    "#### **i. Assumes a Linear Relationship**  \n",
    "- Assumes a **linear relationship** between **features and log-odds**.  \n",
    "- Struggles with **non-linear relationships**.  \n",
    "\n",
    "📌 **Example:** If a dataset has **complex interactions** (e.g., image recognition), Logistic Regression won’t work well.  \n",
    "✅ **Solution:** Use **Polynomial Features** or a more flexible model (e.g., Decision Trees, Neural Networks).  \n",
    "\n",
    "---\n",
    "\n",
    "#### **iii. Struggles with Highly Correlated Features**  \n",
    "- **Multicollinearity** (highly correlated features) affects coefficient interpretation.  \n",
    "\n",
    "📌 **Example:** In housing prices, both **\"Size in sq. ft.\"** and **\"Number of bedrooms\"** are correlated.  \n",
    "✅ **Solution:** Use **PCA (Principal Component Analysis)** or **drop one correlated feature**.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **iii. Poor Performance on Imbalanced Data**  \n",
    "- **Majority class dominates predictions**, leading to low recall for minority class.  \n",
    "\n",
    "📌 **Example:** In fraud detection (1% fraud, 99% non-fraud), the model may **predict \"Not Fraud\" always** and achieve **99% accuracy** but still fail.  \n",
    "✅ **Solution:** Use **class weights (`class_weight='balanced'`)** or **resampling techniques (SMOTE)**.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **iv. Can’t Handle Complex Relationships**  \n",
    "- Doesn’t work well when **features interact in complex, non-linear ways**.  \n",
    "\n",
    "📌 **Example:**  \n",
    "- Recognizing **handwritten digits** (where pixel positions matter) is difficult for Logistic Regression.  \n",
    "✅ **Solution:** Use **Decision Trees, Random Forests, or Deep Learning**.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **v. Sensitive to Outliers**  \n",
    "- Logistic Regression **assumes normally distributed features** and can be **distorted by extreme values**.  \n",
    "\n",
    "📌 **Example:** If a single customer spent **$1,000,000** on an online store, it may skew the model’s decision boundary.  \n",
    "✅ **Solution:** Use **Robust Scaling (`sklearn.preprocessing.RobustScaler`)** or detect outliers using **IQR (Interquartile Range)**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Final Summary: When to Use Logistic Regression?**  \n",
    "\n",
    "| **Pros ✅** | **Cons ❌** |\n",
    "|------------|------------|\n",
    "| Simple & interpretable | Assumes a **linear decision boundary** |\n",
    "| Fast and computationally efficient | Struggles with **non-linear relationships** |\n",
    "| Provides **probability scores** | Sensitive to **imbalanced data** |\n",
    "| Works well with **regularization (L1, L2, Elastic Net)** | Doesn’t perform well on **complex datasets** |\n",
    "| Good for small to medium datasets | Affected by **outliers & multicollinearity** |\n",
    "\n",
    "✅ **Use Logistic Regression when:**  \n",
    "- You need a simple, **interpretable model**.  \n",
    "- The dataset is **small** to **moderately large**.  \n",
    "- Features are **independent & linearly related** to the log-odds.  \n",
    "\n",
    "❌ **Avoid it when:**  \n",
    "- You have **complex relationships** between features.  \n",
    "- Data is **highly imbalanced** or contains **many outliers**.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. ***What are some use cases of Logistic Regression?***\n",
    "*Answer-* \n",
    "\n",
    "Logistic Regression is widely used for **`binary and multiclass classification`** problems. It's simple, interpretable, and effective in many domains. Some real-world applications:\n",
    "\n",
    "---\n",
    "\n",
    "#### **i. Medical Diagnosis**  \n",
    "- **Disease Prediction:** Predict whether a patient has a disease (e.g., **Diabetes, Heart Disease, Cancer**).  \n",
    "- **COVID-19 Risk Assessment:** Classify patients as **low-risk or high-risk**.  \n",
    "- **Sepsis Prediction:** Identify early signs of **sepsis in ICU patients**.  \n",
    "\n",
    "📌 **Example:**  \n",
    "🔹 **Input:** Blood sugar levels, age, BMI.  \n",
    "🔹 **Output:** Diabetes (Yes/No).  \n",
    "\n",
    "🔹 **Implementation:**  \n",
    "```python\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)  # Predict diabetes based on patient data\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **ii. Fraud Detection**  \n",
    "- Detect **fraudulent transactions** in banking and e-commerce.  \n",
    "- Identify **fake reviews** or **spam emails**.  \n",
    "- Classify **suspicious login attempts**.  \n",
    "\n",
    "📌 **Example:**  \n",
    "🔹 **Input:** Transaction amount, location, time of day.  \n",
    "🔹 **Output:** Fraud (Yes/No).  \n",
    "\n",
    "✅ **Handling Class Imbalance:**  \n",
    "- Use `class_weight='balanced'` or **SMOTE** to improve fraud detection.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **iii. Customer Churn Prediction**  \n",
    "- Identify customers likely to **leave a service** (e.g., telecom, banking, subscription-based services).  \n",
    "- Helps businesses **retain high-risk customers**.  \n",
    "\n",
    "📌 **Example:**  \n",
    "🔹 **Input:** Call duration, complaints, monthly bill.  \n",
    "🔹 **Output:** Churn (Yes/No).  \n",
    "\n",
    "---\n",
    "\n",
    "#### **iv. Spam Detection**  \n",
    "- Classify emails as **Spam or Not Spam**.  \n",
    "- Used in **email filtering (e.g., Gmail, Outlook)**.  \n",
    "\n",
    "📌 **Example:**  \n",
    "🔹 **Input:** Keywords, sender reputation, number of links in email.  \n",
    "🔹 **Output:** Spam (Yes/No).  \n",
    "\n",
    "✅ **Alternative Models:** Naïve Bayes, SVM, Deep Learning (RNNs).  \n",
    "\n",
    "---\n",
    "\n",
    "#### **v. Credit Scoring & Loan Approval**  \n",
    "- Predict whether a **loan applicant is likely to default**.  \n",
    "- Used by **banks & financial institutions**.  \n",
    "\n",
    "📌 **Example:**  \n",
    "🔹 **Input:** Income, credit score, employment status.  \n",
    "🔹 **Output:** Loan approval (Yes/No).  \n",
    "\n",
    "✅ **Feature Engineering Tip:** Transform income into logarithmic scale to handle skewness.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **vi. Sentiment Analysis**  \n",
    "- Classify customer/product reviews as **Positive or Negative**.  \n",
    "- Used in **social media monitoring & brand analysis**.  \n",
    "\n",
    "📌 **Example:**  \n",
    "🔹 **Input:** Review text, word count, sentiment score.  \n",
    "🔹 **Output:** Positive (1) or Negative (0).  \n",
    "\n",
    "✅ **Alternative Models:** Random Forest, LSTMs (Deep Learning).  \n",
    "\n",
    "---\n",
    "\n",
    "#### **vii. Employee Attrition Prediction**  \n",
    "- Predict which employees are likely to **leave a company**.  \n",
    "- Helps HR teams **reduce turnover**.  \n",
    "\n",
    "📌 **Example:**  \n",
    "🔹 **Input:** Work hours, salary, promotions.  \n",
    "🔹 **Output:** Attrition (Yes/No).  \n",
    "\n",
    "---\n",
    "\n",
    "#### **viii. Image Classification (Simple Cases)**  \n",
    "- Logistic Regression is used in **basic image classification**, like handwritten digit recognition (**MNIST dataset**).  \n",
    "\n",
    "✅ **Alternative Models:** CNNs (Deep Learning) for complex images.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **ix. Marketing Campaign Effectiveness**  \n",
    "- Predict if a customer will **respond to a marketing campaign**.  \n",
    "- Helps in **personalized advertising**.  \n",
    "\n",
    "📌 **Example:**  \n",
    "🔹 **Input:** Customer age, previous purchases, email engagement.  \n",
    "🔹 **Output:** Conversion (Yes/No).  \n",
    "\n",
    "---\n",
    "\n",
    "#### **x. Voting & Election Forecasting**  \n",
    "- Predict **if a voter will vote for a candidate**.  \n",
    "- Used in **political polling & campaign strategy**.  \n",
    "\n",
    "📌 **Example:**  \n",
    "🔹 **Input:** Age, political affiliation, past voting history.  \n",
    "🔹 **Output:** Vote (Candidate A or B).  \n",
    "\n",
    "---\n",
    "\n",
    "#### **Summary Table: When to Use Logistic Regression?**  \n",
    "\n",
    "| **Use Case** | **Binary or Multiclass?** | **Alternative Models?** |\n",
    "|-------------|-----------------|-----------------|\n",
    "| **Medical Diagnosis** | Binary | Decision Trees, SVM |\n",
    "| **Fraud Detection** | Binary | Random Forest, Neural Networks |\n",
    "| **Churn Prediction** | Binary | XGBoost, Random Forest |\n",
    "| **Spam Detection** | Binary | Naïve Bayes, SVM |\n",
    "| **Loan Approval** | Binary | Decision Trees, XGBoost |\n",
    "| **Sentiment Analysis** | Binary | LSTMs, Transformers |\n",
    "| **Employee Attrition** | Binary | Random Forest, SVM |\n",
    "| **Basic Image Classification** | Multiclass | CNNs |\n",
    "| **Marketing Campaigns** | Binary | XGBoost, Neural Networks |\n",
    "| **Election Forecasting** | Binary | Random Forest |\n",
    "\n",
    "---\n",
    "\n",
    "#### **TL;DR: When Should You Use Logistic Regression?**\n",
    "✅ **Best When:**  \n",
    "- You need a **simple, interpretable model**.  \n",
    "- The dataset is **small to medium-sized**.  \n",
    "- The features are **linearly related** to the log-odds.  \n",
    "\n",
    "#### **Avoid When:**  \n",
    "- Data has **non-linear relationships** (use Decision Trees, Neural Networks).  \n",
    "- The dataset is **highly imbalanced** (use class weighting or resampling).  \n",
    "- There are **many outliers** (consider Robust Regression).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. ***What is the difference between Softmax Regression and Logistic Regression?***\n",
    "*Answer-*\n",
    "\n",
    "### **Softmax Regression vs. Logistic Regression**  \n",
    "\n",
    "Both **Softmax Regression** and **Logistic Regression** are used for classification, but they differ in how they handle **`binary vs. multiclass classification`**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **1️⃣ Logistic Regression (Binary Classification)**\n",
    "- Used when **there are only two classes** (e.g., Spam vs. Not Spam).  \n",
    "- Uses the **sigmoid function** to output a probability between **0 and 1**.  \n",
    " \n",
    "\n",
    "📌 **Example:**  \n",
    "**Medical Diagnosis (Diabetes Prediction)**  \n",
    "🔹 **Input:** Age, BMI, Blood Pressure  \n",
    "🔹 **Output:** **Diabetes (Yes/No)**  \n",
    "\n",
    "---\n",
    "\n",
    "#### **2️⃣ Softmax Regression (Multiclass Classification)**\n",
    "- Used when **there are three or more classes** (e.g., Classify an email as Spam, Promotions, or Social).  \n",
    "- Generalizes Logistic Regression for **multiclass classification**.  \n",
    "- Uses the **softmax function** to compute probabilities for **each class**:  \n",
    "\n",
    "  - Assigns the class with the **highest probability**.  \n",
    "\n",
    "📌 **Example:**  \n",
    "**Digit Recognition (0-9 classification)**  \n",
    "🔹 **Input:** Handwritten digits (image pixels)  \n",
    "🔹 **Output:** One of **10 classes (0-9)**  \n",
    "\n",
    "---\n",
    "\n",
    "## **Key Differences**  \n",
    "\n",
    "| Feature | Logistic Regression | Softmax Regression |\n",
    "|---------|----------------------|----------------------|\n",
    "| **Type** | Binary Classification | Multiclass Classification |\n",
    "| **Output** | Probability for class **1** | Probabilities for **all classes** |\n",
    "| **Activation Function** | Sigmoid (outputs 0 to 1) | Softmax (outputs probabilities for all classes) |\n",
    "| **Decision Rule** | \\( P > 0.5 \\) → Class 1, else Class 0 | Pick the class with the **highest probability** |\n",
    "| **Use Case** | Spam Detection (Spam/Not Spam) | Digit Recognition (0-9) |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Which One Should You Use?**  \n",
    "✅ **Use Logistic Regression when:**  \n",
    "- You have **only two classes** (Yes/No, 0/1).  \n",
    "- Example: **Fraud detection, Loan approval**.  \n",
    "\n",
    "✅ **Use Softmax Regression when:**  \n",
    "- You have **three or more classes**.  \n",
    "- Example: **Handwritten digit recognition, Sentiment classification (Positive/Neutral/Negative)**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. ***How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?***\n",
    "*Answer-*\n",
    "\n",
    "#### **Choosing Between One-vs-Rest (OvR) and Softmax for Multiclass Classification**  \n",
    "\n",
    "When dealing with **multiclass classification**, we have two common strategies:  \n",
    "\n",
    "1️⃣ **`One-vs-Rest (OvR) Logistic Regression.`**  \n",
    "2️⃣ **`Softmax (Multinomial) Logistic Regression.`**  \n",
    "\n",
    "Both are used in different scenarios.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **1️⃣ One-vs-Rest (OvR) Logistic Regression**  \n",
    "#### **How it Works**  \n",
    "- Trains **one binary classifier per class**.  \n",
    "- Each classifier separates **one class vs. the rest**.  \n",
    "- The class with the **highest probability** is chosen.  \n",
    "\n",
    "📌 **Example (Digit Recognition: 0-9)**  \n",
    "- Classifier 1: **Digit 0 vs. (1,2,3,...9)**  \n",
    "- Classifier 2: **Digit 1 vs. (0,2,3,...9)**  \n",
    "- ...  \n",
    "- Classifier 10: **Digit 9 vs. (0,1,2,...8)**  \n",
    "\n",
    "#### **When to Use OvR?**  \n",
    "✅ **Best when you have many classes (K > 3-4)**.  \n",
    "✅ Works well with **linear classifiers (e.g., SVM, Logistic Regression)**.  \n",
    "✅ Easier to implement and computationally **cheaper for large datasets**.  \n",
    "\n",
    "🔴 **Downsides**  \n",
    "- Classifiers are trained **independently**, leading to **inconsistent probabilities**.  \n",
    "- Can be **inefficient** if the number of classes is very large.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **2️⃣ Softmax (Multinomial) Logistic Regression**  \n",
    "#### **How it Works**  \n",
    "- Uses a **single model** that assigns probabilities to **all classes at once**.  \n",
    "- Uses the **softmax function** to normalize probabilities.  \n",
    "- Directly optimizes for **all classes together**, leading to more **consistent results**.  \n",
    "\n",
    "📌 **Example (Movie Genre Classification: Action, Comedy, Drama)**  \n",
    "- Instead of training 3 separate classifiers, Softmax **outputs 3 probabilities at once**.  \n",
    "\n",
    "#### **When to Use Softmax?**  \n",
    "✅ Works best when **classes are mutually exclusive** (only one correct class).  \n",
    "✅ More **probabilistically sound** since all probabilities sum to **1**.  \n",
    "✅ **Preferred when using deep learning (Neural Networks, TensorFlow, PyTorch).**  \n",
    "\n",
    "🔴 **Downsides**  \n",
    "- Requires more **complex optimization** than OvR.  \n",
    "- Computationally more **expensive** for a very large number of classes.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **OvR vs. Softmax: Key Differences**  \n",
    "\n",
    "| Feature | **One-vs-Rest (OvR)** | **Softmax (Multinomial)** |\n",
    "|---------|------------------|------------------|\n",
    "| **Number of models** | **K** binary classifiers | **1** model for all classes |\n",
    "| **Computational Cost** | Cheaper for **small K** | More expensive for **large K** |\n",
    "| **Probability Interpretation** | Each class independently predicts | Probabilities sum to **1** (better calibration) |\n",
    "| **Best for** | Many classes (e.g., Image Classification) | Mutually exclusive classes |\n",
    "| **Works well with** | SVM, Logistic Regression | Deep Learning (Neural Nets) |\n",
    "| **Drawback** | Inconsistent probabilities | Computationally heavier |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Which One Should You Choose?**  \n",
    "\n",
    "✅ **Use OvR when:**  \n",
    "- **K is large (K > 3-4 classes).**  \n",
    "- We’re using **SVMs or simple models like Logistic Regression**.  \n",
    "- We want **faster training and inference**.  \n",
    "\n",
    "✅ **Use Softmax when:**  \n",
    "- **Classes are mutually exclusive** (e.g., single-label classification).  \n",
    "- We need **well-calibrated probabilities**.  \n",
    "- We’re working with **deep learning models (Neural Networks, CNNs, etc.)**.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **Final Thoughts**  \n",
    "- When using **Logistic Regression or SVM**, **go with OvR**.  \n",
    "- If need **better probability estimates** or are working with **Neural Networks**, **use Softmax**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OvR Accuracy: 0.9666666666666667\n",
      "Softmax Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#Example >>\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target  # Three classes: 0, 1, 2\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# One-vs-Rest (OvR) Logistic Regression\n",
    "ovr_model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=200)\n",
    "ovr_model.fit(X_train, y_train)\n",
    "y_pred_ovr = ovr_model.predict(X_test)\n",
    "\n",
    "# Softmax (Multinomial) Logistic Regression\n",
    "softmax_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)\n",
    "softmax_model.fit(X_train, y_train)\n",
    "y_pred_softmax = softmax_model.predict(X_test)\n",
    "\n",
    "# Compare Accuracy\n",
    "print(\"OvR Accuracy:\", accuracy_score(y_test, y_pred_ovr))\n",
    "print(\"Softmax Accuracy:\", accuracy_score(y_test, y_pred_softmax))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. ***How do we interpret coefficients in Logistic Regression?***\n",
    "*Answer-*\n",
    "\n",
    "In Logistic Regression, **coefficients represent** the change in the `\"log odds\"` of the outcome variable for a one-unit increase in the corresponding independent variable, holding all other variables constant; to interpret them more intuitively, it typically **`exponentiate the coefficient to get the \"odds ratio\", which tells us how much the odds of the event increase or decrease for a one-unit change in the predictor variable.`**\n",
    "\n",
    "#### **`Key points about interpreting Logistic Regression coefficients:`**\n",
    "\n",
    "#### **• Log odds scale:**\n",
    "\n",
    "-   The coefficients themselves are on the log odds scale, meaning a positive coefficient indicates an increased odds of the event occurring with a one-unit increase in the predictor, while a negative coefficient indicates a decreased odds.\n",
    "\n",
    "#### **• Odds Ratio:**\n",
    "\n",
    "-   To interpret the effect in a more understandable way, exponentiate the coefficient to get the odds ratio. An odds ratio greater than 1 means the event is more likely to occur with a one-unit increase in the predictor. \n",
    "\n",
    "• Example:\n",
    "\n",
    "-   If the coefficient for \"age\" is 0.05, and you exponentiate it to get 1.05, it means that for every one year increase in age, the odds of the event occurring increase by 5%.\n",
    "\n",
    "#### **`Important considerations when interpreting coefficients:`**\n",
    "\n",
    "#### **• Statistical significance:**\n",
    "-   Always check the p-value associated with each coefficient to determine if the observed effect is statistically significant.\n",
    "\n",
    "#### **• Interaction effects:**\n",
    "-   If your model includes interaction terms, the interpretation of individual coefficients becomes more complex and needs to be done considering the combined effect of interacting variables.\n",
    "\n",
    "#### **• Categorical variables:**\n",
    "-   When dealing with categorical variables, the reference level is crucial for interpretation.\n",
    "\n",
    "#### **Short notes:**\n",
    "✅ Logistic Regression coefficients represent log-odds, not probabilities.\n",
    "\n",
    "✅ Exponentiate (𝑒𝛽) to get the odds ratio for easy interpretation.\n",
    "\n",
    "✅ Positive coefficients → Increase in event probability (higher risk).\n",
    "\n",
    "✅ Negative coefficients → Decrease in event probability (protective effect).\n",
    "\n",
    "✅ Feature scaling helps in making coefficients comparable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Feature  Coefficient  Odds Ratio\n",
      "0             age     0.655224    1.925573\n",
      "1     cholesterol     0.795421    2.215373\n",
      "2  exercise_hours    -0.771433    0.462350\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data = {\n",
    "    'age': [25, 45, 35, 50, 23, 40, 30, 60],\n",
    "    'cholesterol': [180, 240, 200, 260, 170, 230, 190, 280],\n",
    "    'exercise_hours': [3, 1, 2, 0, 4, 1, 3, 0],\n",
    "    'heart_disease': [0, 1, 0, 1, 0, 1, 0, 1]  # Target variable\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# features and target\n",
    "X = df[['age', 'cholesterol', 'exercise_hours']]\n",
    "y = df['heart_disease']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_scaled, y)\n",
    "\n",
    "# Extract coefficients\n",
    "coefficients = model.coef_[0]\n",
    "odds_ratios = np.exp(coefficients)\n",
    "\n",
    "# DataFrame for interpretation\n",
    "coef_df = pd.DataFrame({'Feature': X.columns, 'Coefficient': coefficients, 'Odds Ratio': odds_ratios})\n",
    "print(coef_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
